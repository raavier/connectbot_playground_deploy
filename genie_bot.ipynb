{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genie Bot - Development Notebook\n",
    "\n",
    "Notebook para desenvolvimento e teste do Genie Bot linha a linha"
   ]
  },
  {
   "cell_type": "code",
   "source": "%pip install -U -qqqq databricks-openai backoff databricks-agents mlflow-skinny[databricks] unitycatalog-ai\ndbutils.library.restartPython()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "import json\nimport requests\nfrom typing import Any, Callable, Generator\nfrom uuid import uuid4\n\nimport mlflow\nfrom databricks.sdk import WorkspaceClient\nfrom mlflow.entities import SpanType\nfrom mlflow.pyfunc import ResponsesAgent\nfrom mlflow.types.responses import (\n    ResponsesAgentRequest,\n    ResponsesAgentResponse,\n    ResponsesAgentStreamEvent,\n    output_to_responses_items_stream,\n    to_chat_completions_input,\n)\nfrom pydantic import BaseModel",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configurações\nLLM_ENDPOINT_NAME = \"databricks-llama-4-maverick\"\nGENIE_SPACE_ID = \"SEU_SPACE_ID_AQUI\"  # ← Substituir pelo seu Space ID\n\nSYSTEM_PROMPT = \"\"\"Você é um assistente que responde perguntas sobre dados CRM usando o Genie Space.\nPara qualquer pergunta sobre dados, use a ferramenta query_genie para obter a resposta.\"\"\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações\n",
    "\n",
    "**IMPORTANTE**: Edite o GENIE_SPACE_ID abaixo com o ID do seu Genie Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações\n",
    "LLM_ENDPOINT_NAME = \"databricks-llama-4-maverick\"\n",
    "GENIE_SPACE_ID = \"SEU_SPACE_ID_AQUI\"  # ← Substituir pelo seu Space ID\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Você é um assistente que responde perguntas sobre dados CRM usando o Genie Space.\n",
    "Para qualquer pergunta sobre dados, use a ferramenta query_genie para obter a resposta.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Definir a tool query_genie como função Python nativa\ndef query_genie_tool(question: str) -> str:\n    \"\"\"\n    Query the Genie Space with a natural language question about CRM data.\n\n    Args:\n        question: Natural language question to ask Genie\n\n    Returns:\n        Answer from Genie Space with data and insights\n    \"\"\"\n    try:\n        # Obter workspace client (já tem autenticação)\n        workspace_client = WorkspaceClient()\n\n        # Pegar host e token do workspace client\n        api_client = workspace_client.api_client\n        host = api_client.host\n        token = api_client.token\n\n        # API endpoint\n        url = f\"{host}/api/2.0/genie/spaces/{GENIE_SPACE_ID}/start-conversation\"\n\n        headers = {\n            \"Authorization\": f\"Bearer {token}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        payload = {\"content\": question}\n\n        response = requests.post(url, headers=headers, json=payload, timeout=60)\n        response.raise_for_status()\n        result = response.json()\n\n        # Extrair resposta do Genie\n        if isinstance(result, dict):\n            # Tentar pegar o conteúdo da mensagem\n            if \"message\" in result and isinstance(result[\"message\"], dict):\n                if \"content\" in result[\"message\"]:\n                    return result[\"message\"][\"content\"]\n\n            # Tentar pegar attachments (query results)\n            if \"attachments\" in result:\n                attachments = result[\"attachments\"]\n                if attachments and len(attachments) > 0:\n                    first_attachment = attachments[0]\n                    if \"text\" in first_attachment:\n                        return first_attachment[\"text\"][\"content\"]\n                    if \"query\" in first_attachment:\n                        query_result = first_attachment[\"query\"]\n                        if \"result\" in query_result:\n                            return json.dumps(query_result[\"result\"], ensure_ascii=False, indent=2)\n\n            # Se não encontrou, retornar o JSON completo\n            return json.dumps(result, ensure_ascii=False, indent=2)\n\n        return str(result)\n\n    except requests.exceptions.Timeout:\n        return \"Error: Genie query timed out after 60 seconds. Try a simpler question.\"\n    except requests.exceptions.RequestException as e:\n        return f\"Error calling Genie API: {str(e)}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\nprint(\"query_genie_tool function defined successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Create Tool Specs\n\n**Não precisa mais de UC Function!** A tool agora é uma função Python nativa."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Criar tool spec manualmente\nTOOL_INFOS = [\n    ToolInfo(\n        name=\"query_genie\",\n        spec={\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"query_genie\",\n                \"description\": \"Query the Genie Space with a natural language question about CRM data. Use this for any questions about verifications, risks, deviations, users, actions, or any CRM metrics.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"question\": {\n                            \"type\": \"string\",\n                            \"description\": \"Natural language question to ask Genie about CRM data\"\n                        }\n                    },\n                    \"required\": [\"question\"]\n                }\n            }\n        },\n        exec_fn=query_genie_tool\n    )\n]\n\nprint(f\"Loaded {len(TOOL_INFOS)} tools:\")\nfor tool in TOOL_INFOS:\n    print(f\"  - {tool.name}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load UC Function Tools\n",
    "\n",
    "**Pré-requisito**: A UC Function `hs_franquia.gold_connect_bot.query_genie` deve estar criada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UC Function: query_genie\n",
    "UC_TOOL_NAMES = [\"hs_franquia.gold_connect_bot.query_genie\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=UC_TOOL_NAMES)\n",
    "TOOL_INFOS = [create_tool_info(tool_spec) for tool_spec in uc_toolkit.tools]\n",
    "\n",
    "print(f\"Loaded {len(TOOL_INFOS)} tools:\")\n",
    "for tool in TOOL_INFOS:\n",
    "    print(f\"  - {tool.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenieBot Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenieBot(ResponsesAgent):\n",
    "    def __init__(self, llm_endpoint: str, tools: list[ToolInfo]):\n",
    "        self.llm_endpoint = llm_endpoint\n",
    "        self.workspace_client = WorkspaceClient()\n",
    "        self.model_serving_client = self.workspace_client.serving_endpoints.get_open_ai_client()\n",
    "        self._tools_dict = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def get_tool_specs(self) -> list[dict]:\n",
    "        return [tool_info.spec for tool_info in self._tools_dict.values()]\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.TOOL)\n",
    "    def execute_tool(self, tool_name: str, args: dict) -> Any:\n",
    "        return self._tools_dict[tool_name].exec_fn(**args)\n",
    "\n",
    "    def call_llm(self, messages: list[dict[str, Any]]) -> Generator[dict[str, Any], None, None]:\n",
    "        for chunk in self.model_serving_client.chat.completions.create(\n",
    "            model=self.llm_endpoint,\n",
    "            messages=to_chat_completions_input(messages),\n",
    "            tools=self.get_tool_specs(),\n",
    "            stream=True,\n",
    "        ):\n",
    "            chunk_dict = chunk.to_dict()\n",
    "            if len(chunk_dict.get(\"choices\", [])) > 0:\n",
    "                yield chunk_dict\n",
    "\n",
    "    def handle_tool_call(self, tool_call: dict[str, Any], messages: list[dict[str, Any]]) -> ResponsesAgentStreamEvent:\n",
    "        args = json.loads(tool_call[\"arguments\"])\n",
    "        result = str(self.execute_tool(tool_name=tool_call[\"name\"], args=args))\n",
    "\n",
    "        tool_call_output = self.create_function_call_output_item(tool_call[\"call_id\"], result)\n",
    "        messages.append(tool_call_output)\n",
    "        return ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=tool_call_output)\n",
    "\n",
    "    def call_and_run_tools(self, messages: list[dict[str, Any]], max_iter: int = 10) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        for _ in range(max_iter):\n",
    "            last_msg = messages[-1]\n",
    "            if last_msg.get(\"role\", None) == \"assistant\":\n",
    "                return\n",
    "            elif last_msg.get(\"type\", None) == \"function_call\":\n",
    "                yield self.handle_tool_call(last_msg, messages)\n",
    "            else:\n",
    "                yield from output_to_responses_items_stream(\n",
    "                    chunks=self.call_llm(messages), aggregator=messages\n",
    "                )\n",
    "\n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_text_output_item(\"Max iterations reached.\", str(uuid4())),\n",
    "        )\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    def predict_stream(self, request: ResponsesAgentRequest) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        messages = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        if SYSTEM_PROMPT:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "        yield from self.call_and_run_tools(messages=messages)\n",
    "\n",
    "print(\"GenieBot class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar agent\n",
    "mlflow.openai.autolog()\n",
    "AGENT = GenieBot(llm_endpoint=LLM_ENDPOINT_NAME, tools=TOOL_INFOS)\n",
    "\n",
    "print(f\"Agent initialized with:\")\n",
    "print(f\"  - LLM endpoint: {LLM_ENDPOINT_NAME}\")\n",
    "print(f\"  - Tools: {list(AGENT._tools_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste simples\n",
    "test_request = {\n",
    "    \"input\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Quantas verificações tivemos em 2024?\"\n",
    "    }]\n",
    "}\n",
    "\n",
    "result = AGENT.predict(test_request)\n",
    "print(\"Response:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Extract Text from Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair texto da resposta\n",
    "if result and hasattr(result, 'output') and len(result.output) > 0:\n",
    "    last_output = result.output[-1]\n",
    "    if hasattr(last_output, 'content'):\n",
    "        print(\"\\nResposta formatada:\")\n",
    "        print(last_output.content)\n",
    "    else:\n",
    "        print(\"\\nOutput completo:\")\n",
    "        print(last_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com streaming\n",
    "print(\"Testing streaming response...\\n\")\n",
    "\n",
    "for event in AGENT.predict_stream(test_request):\n",
    "    print(f\"Event type: {event.type}\")\n",
    "    if event.type == \"response.output_item.done\":\n",
    "        print(f\"Content: {event.item}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com múltiplas queries\n",
    "test_queries = [\n",
    "    \"Quantas verificações tivemos em 2024?\",\n",
    "    \"Quais são os top 10 riscos com mais desvios?\",\n",
    "    \"Me mostre os usuários com mais ações em aberto\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    result = AGENT.predict({\n",
    "        \"input\": [{\"role\": \"user\", \"content\": query}]\n",
    "    })\n",
    "    \n",
    "    if result and hasattr(result, 'output') and len(result.output) > 0:\n",
    "        last_output = result.output[-1]\n",
    "        if hasattr(last_output, 'content'):\n",
    "            print(last_output.content)\n",
    "        else:\n",
    "            print(last_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Conversation Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com contexto de conversa\n",
    "conversation = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Quantas verificações em 2024?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Foram realizadas 3877 verificações em 2024.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"E em 2023?\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Testing conversation with context...\\n\")\n",
    "result = AGENT.predict(conversation)\n",
    "\n",
    "if result and hasattr(result, 'output') and len(result.output) > 0:\n",
    "    last_output = result.output[-1]\n",
    "    if hasattr(last_output, 'content'):\n",
    "        print(last_output.content)\n",
    "    else:\n",
    "        print(last_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Debug: Test Genie Tool Directly"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Debug: Testar query_genie_tool diretamente\nresult = query_genie_tool(\"Quantas verificações em 2024?\")\n\nprint(\"Genie tool result:\")\nprint(result)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: Test UC Function Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Testar UC Function diretamente\n",
    "from unitycatalog.ai.core.base import get_uc_function_client\n",
    "\n",
    "uc_client = get_uc_function_client()\n",
    "result = uc_client.execute_function(\n",
    "    \"hs_franquia.gold_connect_bot.query_genie\",\n",
    "    {\n",
    "        \"question\": \"Quantas verificações em 2024?\",\n",
    "        \"space_id\": GENIE_SPACE_ID\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"UC Function result:\")\n",
    "if result.error:\n",
    "    print(f\"Error: {result.error}\")\n",
    "else:\n",
    "    print(f\"Value: {result.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Model for MLflow\n",
    "\n",
    "Execute esta célula quando estiver pronto para fazer o deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar o modelo para MLflow\n",
    "mlflow.models.set_model(AGENT)\n",
    "print(\"Agent registered with MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Após validar que tudo funciona:\n",
    "1. Execute as células acima em ordem\n",
    "2. Valide que os testes retornam respostas corretas do Genie\n",
    "3. Use `deploy_genie_bot.ipynb` para fazer o deployment completo\n",
    "\n",
    "Ou copie o conteúdo deste notebook para `genie_bot.py` usando `%%writefile` se preferir deploy via arquivo Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}